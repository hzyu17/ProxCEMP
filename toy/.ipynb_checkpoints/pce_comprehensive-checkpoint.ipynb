{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCE vs NGD vs CasADi: A Comprehensive Comparison\n",
    "\n",
    "This notebook demonstrates the progression from simple to complex optimization problems,\n",
    "and shows how different algorithms handle increasing nonlinearity.\n",
    "\n",
    "## Outline\n",
    "1. **Section 1: Simple Quadratic** - All methods work well\n",
    "2. **Section 2: Quadratic + Two Barriers** - Local minima traps\n",
    "3. **Section 3: PCE Stability** - Naive vs Best-Preserving PCE\n",
    "4. **Section 4: Covariance Scheduling** - Compare different schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from ipywidgets import interact, IntSlider, Dropdown, FloatSlider\n",
    "from enum import Enum\n",
    "import casadi as ca\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovarianceSchedule(Enum):\n",
    "    CONSTANT = 0\n",
    "    LINEAR = 1\n",
    "    EXPONENTIAL = 2\n",
    "    COSINE = 3\n",
    "\n",
    "def compute_covariance_scale(iteration, n_iterations, schedule,\n",
    "    cov_scale_initial=1.0, cov_scale_final=0.01, cov_decay_rate=0.95):\n",
    "    \"\"\"Compute covariance scale factor for given iteration.\n",
    "    \n",
    "    Schedules:\n",
    "    - CONSTANT: σ stays at initial value\n",
    "    - LINEAR: σ decreases linearly from initial to final\n",
    "    - EXPONENTIAL: σ = initial * decay_rate^t\n",
    "    - COSINE: σ follows cosine annealing (smooth decrease)\n",
    "    \"\"\"\n",
    "    t = float(iteration - 1)\n",
    "    T = float(n_iterations)\n",
    "    if schedule == CovarianceSchedule.CONSTANT:\n",
    "        return cov_scale_initial\n",
    "    elif schedule == CovarianceSchedule.LINEAR:\n",
    "        return cov_scale_initial + (cov_scale_final - cov_scale_initial) * (t / T)\n",
    "    elif schedule == CovarianceSchedule.EXPONENTIAL:\n",
    "        return max(cov_scale_final, cov_scale_initial * (cov_decay_rate ** t))\n",
    "    elif schedule == CovarianceSchedule.COSINE:\n",
    "        return cov_scale_final + 0.5 * (cov_scale_initial - cov_scale_final) * (1.0 + np.cos(np.pi * t / T))\n",
    "    return cov_scale_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize covariance schedules\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "n_iter = 50\n",
    "iters = np.arange(1, n_iter + 1)\n",
    "\n",
    "for schedule, color, name in [\n",
    "    (CovarianceSchedule.CONSTANT, 'gray', 'Constant'),\n",
    "    (CovarianceSchedule.LINEAR, 'blue', 'Linear'),\n",
    "    (CovarianceSchedule.EXPONENTIAL, 'orange', 'Exponential'),\n",
    "    (CovarianceSchedule.COSINE, 'green', 'Cosine')]:\n",
    "    scales = [compute_covariance_scale(i, n_iter, schedule) for i in iters]\n",
    "    ax.plot(iters, scales, color=color, lw=2, label=name)\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('Covariance Scale', fontsize=12)\n",
    "ax.set_title('Covariance Scheduling Options', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NAIVE PCE (original, can bounce out of minima)\n",
    "# ============================================================================\n",
    "def run_pce_naive(cost_fn, y_init=0.0, sigma_init=2.0, n_samples=100, n_iterations=50,\n",
    "    elite_ratio=0.1, ema_alpha=0.5, temperature=1.5, temperature_final=0.05,\n",
    "    cov_schedule=CovarianceSchedule.COSINE, cov_scale_initial=1.0, cov_scale_final=0.01, cov_decay_rate=0.95):\n",
    "    \n",
    "    y = y_init\n",
    "    n_elites = max(1, int(n_samples * elite_ratio))\n",
    "    \n",
    "    history = {'y': [y], 'sigma': [sigma_init * cov_scale_initial],\n",
    "               'cost': [cost_fn(np.array([y]))[0]],\n",
    "               'temperature': [temperature], 'samples': [], 'sample_costs': [],\n",
    "               'elite_indices': [], 'elite_weights': [], 'weighted_y': []}\n",
    "    \n",
    "    for iteration in range(1, n_iterations + 1):\n",
    "        progress = (iteration - 1) / max(1, n_iterations - 1)\n",
    "        current_temp = temperature * (temperature_final / temperature) ** progress\n",
    "        cov_scale = compute_covariance_scale(iteration, n_iterations, cov_schedule,\n",
    "            cov_scale_initial, cov_scale_final, cov_decay_rate)\n",
    "        effective_sigma = sigma_init * cov_scale\n",
    "        \n",
    "        # Sample around CURRENT y (problem: can drift!)\n",
    "        epsilon = effective_sigma * np.random.randn(n_samples)\n",
    "        samples = y + epsilon\n",
    "        costs = cost_fn(samples)\n",
    "        elite_indices = np.argsort(costs)[:n_elites]\n",
    "        \n",
    "        weights = np.zeros(n_samples)\n",
    "        for m in elite_indices:\n",
    "            weights[m] = np.exp(-costs[m] / current_temp)\n",
    "        weights /= (np.sum(weights[elite_indices]) + 1e-10)\n",
    "        x_weighted = sum(weights[m] * samples[m] for m in elite_indices)\n",
    "        \n",
    "        history['samples'].append(samples.copy())\n",
    "        history['sample_costs'].append(costs.copy())\n",
    "        history['elite_indices'].append(elite_indices.copy())\n",
    "        history['elite_weights'].append(weights[elite_indices].copy())\n",
    "        history['weighted_y'].append(x_weighted)\n",
    "        \n",
    "        y = (1 - ema_alpha) * y + ema_alpha * x_weighted\n",
    "        history['y'].append(y)\n",
    "        history['sigma'].append(effective_sigma)\n",
    "        history['cost'].append(cost_fn(np.array([y]))[0])\n",
    "        history['temperature'].append(current_temp)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STABLE PCE (with best-so-far tracking)\n",
    "# ============================================================================\n",
    "def run_pce_stable(cost_fn, y_init=0.0, sigma_init=2.0, n_samples=100, n_iterations=50,\n",
    "    elite_ratio=0.1, ema_alpha=0.5, temperature=1.5, temperature_final=0.05,\n",
    "    cov_schedule=CovarianceSchedule.COSINE, cov_scale_initial=1.0, cov_scale_final=0.01, cov_decay_rate=0.95):\n",
    "    \n",
    "    y = y_init\n",
    "    y_best = y_init  # KEY: Track best solution\n",
    "    cost_best = cost_fn(np.array([y_init]))[0]\n",
    "    n_elites = max(1, int(n_samples * elite_ratio))\n",
    "    \n",
    "    history = {'y': [y], 'y_best': [y_best], 'sigma': [sigma_init * cov_scale_initial],\n",
    "               'cost': [cost_best], 'cost_best': [cost_best],\n",
    "               'temperature': [temperature], 'samples': [], 'sample_costs': [],\n",
    "               'elite_indices': [], 'elite_weights': [], 'weighted_y': []}\n",
    "    \n",
    "    for iteration in range(1, n_iterations + 1):\n",
    "        progress = (iteration - 1) / max(1, n_iterations - 1)\n",
    "        current_temp = temperature * (temperature_final / temperature) ** progress\n",
    "        cov_scale = compute_covariance_scale(iteration, n_iterations, cov_schedule,\n",
    "            cov_scale_initial, cov_scale_final, cov_decay_rate)\n",
    "        effective_sigma = sigma_init * cov_scale\n",
    "        \n",
    "        # KEY FIX 1: Sample around BEST solution\n",
    "        epsilon = effective_sigma * np.random.randn(n_samples)\n",
    "        samples = y_best + epsilon\n",
    "        costs = cost_fn(samples)\n",
    "        elite_indices = np.argsort(costs)[:n_elites]\n",
    "        \n",
    "        weights = np.zeros(n_samples)\n",
    "        for m in elite_indices:\n",
    "            weights[m] = np.exp(-costs[m] / current_temp)\n",
    "        weights /= (np.sum(weights[elite_indices]) + 1e-10)\n",
    "        x_weighted = sum(weights[m] * samples[m] for m in elite_indices)\n",
    "        \n",
    "        history['samples'].append(samples.copy())\n",
    "        history['sample_costs'].append(costs.copy())\n",
    "        history['elite_indices'].append(elite_indices.copy())\n",
    "        history['elite_weights'].append(weights[elite_indices].copy())\n",
    "        history['weighted_y'].append(x_weighted)\n",
    "        \n",
    "        y_new = (1 - ema_alpha) * y_best + ema_alpha * x_weighted\n",
    "        cost_new = cost_fn(np.array([y_new]))[0]\n",
    "        \n",
    "        # KEY FIX 2: Only update best if improvement\n",
    "        if cost_new < cost_best:\n",
    "            y_best = y_new\n",
    "            cost_best = cost_new\n",
    "        \n",
    "        y = y_new\n",
    "        history['y'].append(y)\n",
    "        history['y_best'].append(y_best)\n",
    "        history['sigma'].append(effective_sigma)\n",
    "        history['cost'].append(cost_new)\n",
    "        history['cost_best'].append(cost_best)\n",
    "        history['temperature'].append(current_temp)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NGD (Natural Gradient Descent) - with covariance scheduling\n",
    "# ============================================================================\n",
    "def run_ngd(cost_fn, y_init=0.0, sigma_init=2.0, n_samples=100, n_iterations=50,\n",
    "    learning_rate=0.1, temperature=1.0,\n",
    "    cov_schedule=CovarianceSchedule.COSINE, cov_scale_initial=1.0, cov_scale_final=0.01, cov_decay_rate=0.95):\n",
    "    \"\"\"Natural Gradient Descent with covariance scheduling.\n",
    "    \n",
    "    The covariance schedule controls the exploration radius:\n",
    "    - Early: large σ for exploration\n",
    "    - Late: small σ for exploitation/refinement\n",
    "    \"\"\"\n",
    "    y = y_init\n",
    "    history = {'y': [y], 'sigma': [sigma_init * cov_scale_initial],\n",
    "               'cost': [cost_fn(np.array([y]))[0]],\n",
    "               'samples': [], 'sample_costs': [], 'gradient': [], 'epsilon': []}\n",
    "    \n",
    "    for iteration in range(1, n_iterations + 1):\n",
    "        # Apply covariance schedule\n",
    "        cov_scale = compute_covariance_scale(iteration, n_iterations, cov_schedule,\n",
    "            cov_scale_initial, cov_scale_final, cov_decay_rate)\n",
    "        effective_sigma = sigma_init * cov_scale\n",
    "        \n",
    "        # Sample perturbations\n",
    "        epsilon = effective_sigma * np.random.randn(n_samples)\n",
    "        samples = y + epsilon\n",
    "        costs = cost_fn(samples)\n",
    "        \n",
    "        # Estimate natural gradient: E[(cost/temp) * epsilon]\n",
    "        natural_gradient = np.mean((costs / temperature) * epsilon)\n",
    "        \n",
    "        history['samples'].append(samples.copy())\n",
    "        history['sample_costs'].append(costs.copy())\n",
    "        history['gradient'].append(natural_gradient)\n",
    "        history['epsilon'].append(epsilon.copy())\n",
    "        \n",
    "        # Update\n",
    "        y = y - learning_rate * natural_gradient\n",
    "        history['y'].append(y)\n",
    "        history['sigma'].append(effective_sigma)\n",
    "        history['cost'].append(cost_fn(np.array([y]))[0])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CasADi (Gradient-based with smooth barrier approximation)\n",
    "# ============================================================================\n",
    "def run_casadi(cost_expr_fn, y_init=0.0, lr_init=0.5, n_iters=100, use_newton=False):\n",
    "    \"\"\"Run CasADi optimization.\n",
    "    cost_expr_fn: function that takes CasADi symbol x and returns cost expression\n",
    "    \"\"\"\n",
    "    x = ca.SX.sym('x')\n",
    "    J = cost_expr_fn(x)\n",
    "    grad_J = ca.gradient(J, x)\n",
    "    hess_J = ca.hessian(J, x)[0]\n",
    "    \n",
    "    cost_fn = ca.Function('J', [x], [J])\n",
    "    grad_fn = ca.Function('dJ', [x], [grad_J])\n",
    "    hess_fn = ca.Function('ddJ', [x], [hess_J])\n",
    "    \n",
    "    y = y_init\n",
    "    history = {'y': [y], 'cost': [float(cost_fn(y))]}\n",
    "    \n",
    "    lr = lr_init\n",
    "    for i in range(n_iters):\n",
    "        g = float(grad_fn(y))\n",
    "        if use_newton:\n",
    "            h = float(hess_fn(y))\n",
    "            if abs(h) > 1e-10:\n",
    "                y_new = y - g / h\n",
    "            else:\n",
    "                y_new = y - lr * g\n",
    "        else:\n",
    "            y_new = y - lr * g\n",
    "            lr *= 0.98\n",
    "        \n",
    "        history['y'].append(y_new)\n",
    "        history['cost'].append(float(cost_fn(y_new)))\n",
    "        \n",
    "        if abs(g) < 1e-10:\n",
    "            break\n",
    "        y = y_new\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Simple Quadratic Cost\n",
    "\n",
    "$$J(x) = (x - 5)^2 + 0.1 x^2$$\n",
    "\n",
    "**Optimal:** $x^* = \\frac{50}{11} \\approx 4.545$, $J^* \\approx 2.27$\n",
    "\n",
    "All methods should work perfectly on this convex problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function: simple quadratic\n",
    "def cost_quadratic(x, x_star=5.0, R=0.1):\n",
    "    return (x - x_star)**2 + R * x**2\n",
    "\n",
    "def casadi_cost_quadratic(x, x_star=5.0, R=0.1):\n",
    "    return (x - x_star)**2 + R * x**2\n",
    "\n",
    "X_STAR = 50 / 11\n",
    "OPT_COST = cost_quadratic(np.array([X_STAR]))[0]\n",
    "print(f\"Section 1: Simple Quadratic\")\n",
    "print(f\"Optimal: x* = {X_STAR:.4f}, J* = {OPT_COST:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all algorithms\n",
    "np.random.seed(42)\n",
    "Y_INIT = 0.0\n",
    "N_ITER = 50\n",
    "\n",
    "pce_quad = run_pce_stable(cost_quadratic, y_init=Y_INIT, sigma_init=2.0, n_iterations=N_ITER)\n",
    "np.random.seed(42)\n",
    "ngd_quad = run_ngd(cost_quadratic, y_init=Y_INIT, sigma_init=2.0, n_iterations=N_ITER)\n",
    "casadi_quad = run_casadi(casadi_cost_quadratic, y_init=Y_INIT, use_newton=True)\n",
    "\n",
    "print(f\"PCE:    y = {pce_quad['y_best'][-1]:.4f}, cost = {pce_quad['cost_best'][-1]:.4f}\")\n",
    "print(f\"NGD:    y = {ngd_quad['y'][-1]:.4f}, cost = {ngd_quad['cost'][-1]:.4f}\")\n",
    "print(f\"CasADi: y = {casadi_quad['y'][-1]:.4f}, cost = {casadi_quad['cost'][-1]:.4f} ({len(casadi_quad['y'])-1} iters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "x_plot = np.linspace(-2, 8, 500)\n",
    "cost_curve = cost_quadratic(x_plot)\n",
    "\n",
    "for ax, (hist, name, color) in zip(axes, \n",
    "    [(pce_quad, 'PCE', 'green'), (ngd_quad, 'NGD', 'blue'), (casadi_quad, 'CasADi', 'purple')]):\n",
    "    ax.plot(x_plot, cost_curve, 'k-', lw=2, label='J(x)')\n",
    "    ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7, label=f'x*={X_STAR:.2f}')\n",
    "    \n",
    "    # Use y_best/cost_best for PCE, y/cost for others\n",
    "    y_traj = hist.get('y_best', hist['y'])\n",
    "    cost_traj = hist.get('cost_best', hist['cost'])\n",
    "    ax.plot(y_traj, cost_traj, color=color, lw=2, alpha=0.7)\n",
    "    ax.scatter([y_traj[-1]], [cost_traj[-1]], c=color, s=200, marker='*', zorder=10)\n",
    "    \n",
    "    ax.set_xlabel('x'); ax.set_ylabel('J(x)')\n",
    "    ax.set_title(f'{name}: y={y_traj[-1]:.3f}, J={cost_traj[-1]:.3f}', fontsize=12, fontweight='bold', color=color)\n",
    "    ax.legend(loc='upper right'); ax.set_xlim(-2, 8); ax.set_ylim(0, 30); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Section 1: Simple Quadratic - All Methods Converge', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Quadratic + Two Barriers\n",
    "\n",
    "$$J(x) = (x - 5)^2 + 0.1 x^2 + 100 \\cdot \\mathbf{1}_{(2,3)}(x) + 100 \\cdot \\mathbf{1}_{(6,8)}(x)$$\n",
    "\n",
    "**Barrier regions:** $x \\in (2, 3)$ and $x \\in (6, 8)$\n",
    "\n",
    "**Optimal:** $x^* \\approx 4.545$ (in the \"safe corridor\" between barriers)\n",
    "\n",
    "**Challenge:** CasADi's smooth approximation creates artificial local minima!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function: quadratic + two barriers\n",
    "def cost_two_barriers(x, x_star=5.0, R=0.1, barrier_val=100.0):\n",
    "    base = (x - x_star)**2 + R * x**2\n",
    "    b1 = np.where((x > 2) & (x < 3), barrier_val, 0.0)\n",
    "    b2 = np.where((x > 6) & (x < 8), barrier_val, 0.0)\n",
    "    return base + b1 + b2\n",
    "\n",
    "def casadi_cost_two_barriers(x, x_star=5.0, R=0.1, k=2):\n",
    "    \"\"\"Smooth approximation - k=2 creates local minima!\"\"\"\n",
    "    base = (x - x_star)**2 + R * x**2\n",
    "    b1 = 100.0 * (1 / (1 + ca.exp(-k*(x - 2)))) * (1 / (1 + ca.exp(k*(x - 3))))\n",
    "    b2 = 100.0 * (1 / (1 + ca.exp(-k*(x - 6)))) * (1 / (1 + ca.exp(k*(x - 8))))\n",
    "    return base + b1 + b2\n",
    "\n",
    "print(f\"Section 2: Quadratic + Two Barriers at (2,3) and (6,8)\")\n",
    "print(f\"Optimal: x* = {X_STAR:.4f}, J* = {OPT_COST:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the smooth approximation problem\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x_plot = np.linspace(-1, 12, 500)\n",
    "\n",
    "ax.plot(x_plot, cost_two_barriers(x_plot), 'k-', lw=3, label='True J(x)')\n",
    "\n",
    "for k, color in [(50, 'purple'), (20, 'blue'), (10, 'cyan'), (5, 'green'), (2, 'orange')]:\n",
    "    smooth = np.array([(xv - 5)**2 + 0.1 * xv**2 + \n",
    "                       100.0 / (1 + np.exp(-k*(xv - 2))) / (1 + np.exp(k*(xv - 3))) +\n",
    "                       100.0 / (1 + np.exp(-k*(xv - 6))) / (1 + np.exp(k*(xv - 8)))\n",
    "                       for xv in x_plot])\n",
    "    ax.plot(x_plot, smooth, color=color, lw=1.5, alpha=0.7, label=f'Smooth k={k}')\n",
    "\n",
    "ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7, label=f'x*={X_STAR:.2f}')\n",
    "ax.axvspan(2, 3, alpha=0.2, color='orange')\n",
    "ax.axvspan(6, 8, alpha=0.2, color='orange')\n",
    "ax.set_xlabel('x', fontsize=12); ax.set_ylabel('J(x)', fontsize=12)\n",
    "ax.set_title('Smooth Barrier Approximations: Lower k = More Local Minima!', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right'); ax.set_xlim(-1, 12); ax.set_ylim(0, 50); ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run from y=9.5 (in local minimum basin for CasADi k=2)\n",
    "np.random.seed(42)\n",
    "Y_INIT = 9.5\n",
    "\n",
    "pce_2b = run_pce_stable(cost_two_barriers, y_init=Y_INIT, sigma_init=3.0, n_iterations=N_ITER)\n",
    "np.random.seed(42)\n",
    "ngd_2b = run_ngd(cost_two_barriers, y_init=Y_INIT, sigma_init=3.0, n_iterations=N_ITER)\n",
    "casadi_2b = run_casadi(casadi_cost_two_barriers, y_init=Y_INIT, lr_init=0.3)\n",
    "\n",
    "casadi_2b_true_cost = [cost_two_barriers(np.array([y]))[0] for y in casadi_2b['y']]\n",
    "\n",
    "print(f\"Starting from y = {Y_INIT} (in local min basin for CasADi k=2)\")\n",
    "print(f\"PCE (stable): y_best = {pce_2b['y_best'][-1]:.4f}, cost_best = {pce_2b['cost_best'][-1]:.4f}\")\n",
    "print(f\"NGD:          y = {ngd_2b['y'][-1]:.4f}, cost = {ngd_2b['cost'][-1]:.4f}\")\n",
    "print(f\"CasADi (k=2): y = {casadi_2b['y'][-1]:.4f}, TRUE cost = {casadi_2b_true_cost[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "x_plot = np.linspace(-2, 12, 500)\n",
    "cost_curve = cost_two_barriers(x_plot)\n",
    "\n",
    "# PCE\n",
    "ax = axes[0]\n",
    "ax.plot(x_plot, cost_curve, 'k-', lw=2, label='J(x)')\n",
    "ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7, label=f'x*={X_STAR:.2f}')\n",
    "ax.axvspan(2, 3, alpha=0.3, color='orange', label='Barriers')\n",
    "ax.axvspan(6, 8, alpha=0.3, color='orange')\n",
    "ax.scatter([Y_INIT], [cost_two_barriers(np.array([Y_INIT]))[0]], c='black', s=100, marker='o', label='Start', zorder=5)\n",
    "ax.plot(pce_2b['y_best'], pce_2b['cost_best'], 'g-', lw=2)\n",
    "ax.scatter([pce_2b['y_best'][-1]], [pce_2b['cost_best'][-1]], c='green', s=200, marker='*', zorder=10)\n",
    "ax.set_title(f'PCE Stable: y={pce_2b[\"y_best\"][-1]:.2f}, J={pce_2b[\"cost_best\"][-1]:.2f}\\n✓ OPTIMAL', \n",
    "             fontsize=11, fontweight='bold', color='green')\n",
    "ax.legend(loc='upper right', fontsize=7); ax.set_xlim(-2, 12); ax.set_ylim(0, 50); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# NGD\n",
    "ax = axes[1]\n",
    "ax.plot(x_plot, cost_curve, 'k-', lw=2)\n",
    "ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7)\n",
    "ax.axvspan(2, 3, alpha=0.3, color='orange')\n",
    "ax.axvspan(6, 8, alpha=0.3, color='orange')\n",
    "ax.scatter([Y_INIT], [cost_two_barriers(np.array([Y_INIT]))[0]], c='black', s=100, marker='o', zorder=5)\n",
    "ax.plot(ngd_2b['y'], ngd_2b['cost'], 'b-', lw=2)\n",
    "ax.scatter([ngd_2b['y'][-1]], [ngd_2b['cost'][-1]], c='blue', s=200, marker='*', zorder=10)\n",
    "ax.set_title(f'NGD: y={ngd_2b[\"y\"][-1]:.2f}, J={ngd_2b[\"cost\"][-1]:.1f}', \n",
    "             fontsize=11, fontweight='bold', color='blue')\n",
    "ax.set_xlim(-2, 12); ax.set_ylim(0, 50); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# CasADi\n",
    "ax = axes[2]\n",
    "ax.plot(x_plot, cost_curve, 'k-', lw=2, label='True J(x)')\n",
    "smooth_cost = np.array([(xv - 5)**2 + 0.1 * xv**2 + \n",
    "                        100.0 / (1 + np.exp(-2*(xv - 2))) / (1 + np.exp(2*(xv - 3))) +\n",
    "                        100.0 / (1 + np.exp(-2*(xv - 6))) / (1 + np.exp(2*(xv - 8)))\n",
    "                        for xv in x_plot])\n",
    "ax.plot(x_plot, smooth_cost, 'purple', lw=1.5, ls='--', alpha=0.5, label='Smooth (k=2)')\n",
    "ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7)\n",
    "ax.axvspan(2, 3, alpha=0.3, color='orange')\n",
    "ax.axvspan(6, 8, alpha=0.3, color='orange')\n",
    "ax.scatter([Y_INIT], [cost_two_barriers(np.array([Y_INIT]))[0]], c='black', s=100, marker='o', zorder=5)\n",
    "ax.plot(casadi_2b['y'], casadi_2b_true_cost, 'purple', lw=2)\n",
    "ax.scatter([casadi_2b['y'][-1]], [casadi_2b_true_cost[-1]], c='purple', s=200, marker='*', zorder=10)\n",
    "ax.set_title(f'CasADi k=2: y={casadi_2b[\"y\"][-1]:.2f}, J={casadi_2b_true_cost[-1]:.1f}\\n✗ LOCAL MIN', \n",
    "             fontsize=11, fontweight='bold', color='purple')\n",
    "ax.legend(loc='upper right', fontsize=7); ax.set_xlim(-2, 12); ax.set_ylim(0, 50); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Section 2: Two Barriers - Starting from y={Y_INIT}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: PCE Stability - Naive vs Best-Preserving\n",
    "\n",
    "**Problem:** Naive PCE can find the optimum but then \"bounce out\" due to:\n",
    "1. Sampling around current (possibly bad) position\n",
    "2. Outliers in barrier regions pulling the weighted mean\n",
    "\n",
    "**Solution:** Best-so-far tracking ensures:\n",
    "- Always sample around the best known solution\n",
    "- Never regress: cost_best is monotonically non-increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Naive vs Stable PCE on two-barrier problem\n",
    "np.random.seed(42)\n",
    "Y_INIT = 9.5\n",
    "\n",
    "pce_naive = run_pce_naive(cost_two_barriers, y_init=Y_INIT, sigma_init=3.0, n_iterations=N_ITER)\n",
    "np.random.seed(42)\n",
    "pce_stable = run_pce_stable(cost_two_barriers, y_init=Y_INIT, sigma_init=3.0, n_iterations=N_ITER)\n",
    "\n",
    "print(f\"Starting from y = {Y_INIT}\")\n",
    "print(f\"\")\n",
    "print(f\"PCE Naive:\")\n",
    "print(f\"  Final y = {pce_naive['y'][-1]:.4f}\")\n",
    "print(f\"  Final cost = {pce_naive['cost'][-1]:.4f}\")\n",
    "print(f\"  Best cost found = {min(pce_naive['cost']):.4f} at iter {np.argmin(pce_naive['cost'])}\")\n",
    "print(f\"\")\n",
    "print(f\"PCE Stable (best-so-far):\")\n",
    "print(f\"  Final y_best = {pce_stable['y_best'][-1]:.4f}\")\n",
    "print(f\"  Final cost_best = {pce_stable['cost_best'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Naive vs Stable\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(2, 3, figure=fig)\n",
    "\n",
    "x_plot = np.linspace(-2, 12, 500)\n",
    "cost_curve = cost_two_barriers(x_plot)\n",
    "\n",
    "# TOP LEFT: Naive PCE cost landscape\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "ax.plot(x_plot, cost_curve, 'k-', lw=2, label='J(x)')\n",
    "ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7)\n",
    "ax.axvspan(2, 3, alpha=0.3, color='orange', label='Barriers')\n",
    "ax.axvspan(6, 8, alpha=0.3, color='orange')\n",
    "ax.plot(pce_naive['y'], pce_naive['cost'], 'coral', lw=2, alpha=0.7)\n",
    "ax.scatter([pce_naive['y'][-1]], [pce_naive['cost'][-1]], c='coral', s=200, marker='*', zorder=10)\n",
    "min_idx = np.argmin(pce_naive['cost'])\n",
    "ax.scatter([pce_naive['y'][min_idx]], [pce_naive['cost'][min_idx]], c='lime', s=150, marker='o', \n",
    "           edgecolors='green', linewidths=2, zorder=9, label=f'Best @ iter {min_idx}')\n",
    "ax.set_title(f'PCE Naive: BOUNCED OUT\\nFinal J={pce_naive[\"cost\"][-1]:.1f}', fontsize=12, fontweight='bold', color='coral')\n",
    "ax.legend(loc='upper right', fontsize=7); ax.set_xlim(-2, 12); ax.set_ylim(0, 50); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# TOP MIDDLE: Stable PCE cost landscape\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "ax.plot(x_plot, cost_curve, 'k-', lw=2, label='J(x)')\n",
    "ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7)\n",
    "ax.axvspan(2, 3, alpha=0.3, color='orange', label='Barriers')\n",
    "ax.axvspan(6, 8, alpha=0.3, color='orange')\n",
    "ax.plot(pce_stable['y'], pce_stable['cost'], 'lightgreen', lw=1, alpha=0.4, label='Raw y')\n",
    "ax.plot(pce_stable['y_best'], pce_stable['cost_best'], 'green', lw=2, label='y_best')\n",
    "ax.scatter([pce_stable['y_best'][-1]], [pce_stable['cost_best'][-1]], c='green', s=200, marker='*', zorder=10)\n",
    "ax.set_title(f'PCE Stable: OPTIMAL\\nFinal J={pce_stable[\"cost_best\"][-1]:.2f}', fontsize=12, fontweight='bold', color='green')\n",
    "ax.legend(loc='upper right', fontsize=7); ax.set_xlim(-2, 12); ax.set_ylim(0, 50); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# TOP RIGHT: Summary\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "ax.axis('off')\n",
    "summary = f'''\n",
    "WHY PCE BOUNCES & HOW TO FIX IT\n",
    "════════════════════════════════════════\n",
    "\n",
    "PROBLEM: Naive PCE samples around CURRENT y\n",
    "\n",
    "• Outliers in barrier → high cost samples\n",
    "• Can pull weighted mean away from optimum\n",
    "• Next iteration samples around worse position\n",
    "• Drift away even after finding good solution\n",
    "\n",
    "════════════════════════════════════════\n",
    "\n",
    "SOLUTION: Best-so-far tracking\n",
    "\n",
    "1. Sample around y_best (not current y)\n",
    "2. Only update y_best if cost improves\n",
    "3. cost_best is monotonically non-increasing\n",
    "\n",
    "════════════════════════════════════════\n",
    "\n",
    "RESULTS (from y={Y_INIT}):\n",
    "\n",
    "PCE Naive:  J = {pce_naive['cost'][-1]:.1f}  ✗ BOUNCED\n",
    "PCE Stable: J = {pce_stable['cost_best'][-1]:.2f}  ✓ OPTIMAL\n",
    "'''\n",
    "ax.text(0.05, 0.95, summary, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))\n",
    "\n",
    "# BOTTOM LEFT: y trajectory comparison\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax.plot(pce_naive['y'], 'coral', lw=2, label='Naive y')\n",
    "ax.axhline(X_STAR, color='red', ls='--', lw=1.5, alpha=0.7, label='x*')\n",
    "ax.axhspan(2, 3, alpha=0.15, color='orange')\n",
    "ax.axhspan(6, 8, alpha=0.15, color='orange')\n",
    "ax.scatter([min_idx], [pce_naive['y'][min_idx]], c='lime', s=100, zorder=5)\n",
    "ax.set_xlabel('Iteration'); ax.set_ylabel('y')\n",
    "ax.set_title('Naive: y bounces back after finding min', fontsize=11, fontweight='bold', color='coral')\n",
    "ax.legend(loc='upper right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# BOTTOM MIDDLE: Stable y trajectory\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "ax.plot(pce_stable['y'], 'lightgreen', lw=1, alpha=0.4, label='Raw y')\n",
    "ax.plot(pce_stable['y_best'], 'green', lw=2, label='y_best')\n",
    "ax.axhline(X_STAR, color='red', ls='--', lw=1.5, alpha=0.7, label='x*')\n",
    "ax.axhspan(2, 3, alpha=0.15, color='orange')\n",
    "ax.axhspan(6, 8, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Iteration'); ax.set_ylabel('y')\n",
    "ax.set_title('Stable: y_best never regresses', fontsize=11, fontweight='bold', color='green')\n",
    "ax.legend(loc='upper right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# BOTTOM RIGHT: Cost convergence\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "ax.plot(pce_naive['cost'], 'coral', lw=2, label='Naive cost')\n",
    "ax.plot(pce_stable['cost'], 'lightgreen', lw=1, alpha=0.4, label='Stable raw')\n",
    "ax.plot(pce_stable['cost_best'], 'green', lw=2, label='Stable cost_best')\n",
    "ax.axhline(OPT_COST, color='red', ls='--', lw=1.5, alpha=0.7, label=f'Opt={OPT_COST:.2f}')\n",
    "ax.scatter([min_idx], [pce_naive['cost'][min_idx]], c='lime', s=100, zorder=5)\n",
    "ax.set_xlabel('Iteration'); ax.set_ylabel('Cost')\n",
    "ax.set_title('Cost: Naive increases, Stable monotonic ↓', fontsize=11, fontweight='bold')\n",
    "ax.set_yscale('log'); ax.legend(loc='upper right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Section 3: PCE Stability - Naive vs Best-Preserving', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Covariance Scheduling Comparison\n",
    "\n",
    "Compare how different covariance schedules affect PCE and NGD convergence.\n",
    "\n",
    "**Schedules:**\n",
    "- **Constant:** σ stays fixed (good for exploration, may not converge tightly)\n",
    "- **Linear:** σ decreases linearly (simple annealing)\n",
    "- **Exponential:** σ = σ₀ × decay^t (fast initial decay)\n",
    "- **Cosine:** Smooth decrease following cosine curve (best of both worlds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare schedules on the two-barrier problem\n",
    "Y_INIT = 9.5\n",
    "N_ITER = 50\n",
    "SIGMA = 3.0\n",
    "\n",
    "schedules = [\n",
    "    (CovarianceSchedule.CONSTANT, 'Constant', 'gray'),\n",
    "    (CovarianceSchedule.LINEAR, 'Linear', 'blue'),\n",
    "    (CovarianceSchedule.EXPONENTIAL, 'Exponential', 'orange'),\n",
    "    (CovarianceSchedule.COSINE, 'Cosine', 'green')\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# Run and collect results\n",
    "pce_results = {}\n",
    "ngd_results = {}\n",
    "\n",
    "for schedule, name, color in schedules:\n",
    "    np.random.seed(42)\n",
    "    pce_results[name] = run_pce_stable(cost_two_barriers, y_init=Y_INIT, sigma_init=SIGMA,\n",
    "                                        n_iterations=N_ITER, cov_schedule=schedule)\n",
    "    np.random.seed(42)\n",
    "    ngd_results[name] = run_ngd(cost_two_barriers, y_init=Y_INIT, sigma_init=SIGMA,\n",
    "                                 n_iterations=N_ITER, cov_schedule=schedule)\n",
    "\n",
    "# TOP LEFT: PCE cost convergence\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "for schedule, name, color in schedules:\n",
    "    ax.plot(pce_results[name]['cost_best'], color=color, lw=2, label=f'{name}: {pce_results[name][\"cost_best\"][-1]:.2f}')\n",
    "ax.axhline(OPT_COST, color='red', ls='--', lw=1.5, alpha=0.7, label=f'Optimal: {OPT_COST:.2f}')\n",
    "ax.set_xlabel('Iteration'); ax.set_ylabel('Cost')\n",
    "ax.set_title('PCE: Cost Convergence by Schedule', fontsize=12, fontweight='bold')\n",
    "ax.set_yscale('log'); ax.legend(loc='upper right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# TOP RIGHT: NGD cost convergence\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "for schedule, name, color in schedules:\n",
    "    ax.plot(ngd_results[name]['cost'], color=color, lw=2, label=f'{name}: {ngd_results[name][\"cost\"][-1]:.2f}')\n",
    "ax.axhline(OPT_COST, color='red', ls='--', lw=1.5, alpha=0.7, label=f'Optimal: {OPT_COST:.2f}')\n",
    "ax.set_xlabel('Iteration'); ax.set_ylabel('Cost')\n",
    "ax.set_title('NGD: Cost Convergence by Schedule', fontsize=12, fontweight='bold')\n",
    "ax.set_yscale('log'); ax.legend(loc='upper right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# BOTTOM LEFT: PCE sigma evolution\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "for schedule, name, color in schedules:\n",
    "    ax.plot(pce_results[name]['sigma'], color=color, lw=2, label=name)\n",
    "ax.set_xlabel('Iteration'); ax.set_ylabel('σ')\n",
    "ax.set_title('PCE: σ Evolution by Schedule', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# BOTTOM RIGHT: NGD sigma evolution\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "for schedule, name, color in schedules:\n",
    "    ax.plot(ngd_results[name]['sigma'], color=color, lw=2, label=name)\n",
    "ax.set_xlabel('Iteration'); ax.set_ylabel('σ')\n",
    "ax.set_title('NGD: σ Evolution by Schedule', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper right'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Section 4: Covariance Scheduling Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COVARIANCE SCHEDULE COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOptimal: x*={X_STAR:.4f}, J*={OPT_COST:.4f}\")\n",
    "print(f\"\\n{'Schedule':<15} {'PCE Final Cost':<18} {'NGD Final Cost':<18}\")\n",
    "print(\"-\"*51)\n",
    "for schedule, name, color in schedules:\n",
    "    pce_cost = pce_results[name]['cost_best'][-1]\n",
    "    ngd_cost = ngd_results[name]['cost'][-1]\n",
    "    print(f\"{name:<15} {pce_cost:<18.4f} {ngd_cost:<18.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Interactive Exploration\n",
    "\n",
    "Explore different scenarios with adjustable parameters including covariance scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_comparison(y_init, sigma, n_iterations, problem, cov_schedule_name, cov_decay_rate):\n",
    "    # Select cost function\n",
    "    if problem == 'Quadratic':\n",
    "        cost_fn = cost_quadratic\n",
    "        casadi_fn = casadi_cost_quadratic\n",
    "        barriers = []\n",
    "    else:  # Two Barriers\n",
    "        cost_fn = cost_two_barriers\n",
    "        casadi_fn = casadi_cost_two_barriers\n",
    "        barriers = [(2, 3), (6, 8)]\n",
    "    \n",
    "    # Map schedule name to enum\n",
    "    schedule_map = {'Constant': CovarianceSchedule.CONSTANT, 'Linear': CovarianceSchedule.LINEAR,\n",
    "                    'Exponential': CovarianceSchedule.EXPONENTIAL, 'Cosine': CovarianceSchedule.COSINE}\n",
    "    cov_schedule = schedule_map[cov_schedule_name]\n",
    "    \n",
    "    # Run algorithms\n",
    "    np.random.seed(42)\n",
    "    pce = run_pce_stable(cost_fn, y_init=y_init, sigma_init=sigma, n_iterations=n_iterations,\n",
    "                         cov_schedule=cov_schedule, cov_decay_rate=cov_decay_rate)\n",
    "    np.random.seed(42)\n",
    "    ngd = run_ngd(cost_fn, y_init=y_init, sigma_init=sigma, n_iterations=n_iterations,\n",
    "                  cov_schedule=cov_schedule, cov_decay_rate=cov_decay_rate)\n",
    "    casadi = run_casadi(casadi_fn, y_init=y_init)\n",
    "    casadi_true = [cost_fn(np.array([y]))[0] for y in casadi['y']]\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = GridSpec(3, 3, figure=fig, height_ratios=[1.2, 0.8, 0.8])\n",
    "    x_plot = np.linspace(-2, 12, 500)\n",
    "    cost_curve = cost_fn(x_plot)\n",
    "    \n",
    "    # Top row: Cost landscape\n",
    "    for idx, (hist, true_cost, name, color) in enumerate([\n",
    "        (pce, pce['cost_best'], 'PCE Stable', 'green'),\n",
    "        (ngd, ngd['cost'], 'NGD', 'blue'),\n",
    "        (casadi, casadi_true, 'CasADi', 'purple')]):\n",
    "        ax = fig.add_subplot(gs[0, idx])\n",
    "        ax.plot(x_plot, cost_curve, 'k-', lw=2)\n",
    "        ax.axvline(X_STAR, color='red', ls='--', lw=2, alpha=0.7)\n",
    "        for b in barriers:\n",
    "            ax.axvspan(b[0], b[1], alpha=0.3, color='orange')\n",
    "        ax.scatter([y_init], [cost_fn(np.array([y_init]))[0]], c='black', s=100, marker='o', zorder=5)\n",
    "        \n",
    "        y_traj = hist['y_best'] if 'y_best' in hist else hist['y']\n",
    "        ax.plot(y_traj, true_cost, color=color, lw=2)\n",
    "        ax.scatter([y_traj[-1]], [true_cost[-1]], c=color, s=200, marker='*', zorder=10)\n",
    "        \n",
    "        ax.set_title(f'{name}\\ny={y_traj[-1]:.2f}, J={true_cost[-1]:.2f}', fontsize=11, fontweight='bold', color=color)\n",
    "        ax.set_xlim(-2, 12); ax.set_ylim(0, 60); ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Middle row: Cost convergence\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax.plot(pce['cost_best'], 'g-', lw=2)\n",
    "    ax.axhline(OPT_COST, color='red', ls='--', alpha=0.7, label=f'Opt={OPT_COST:.2f}')\n",
    "    ax.set_yscale('log'); ax.set_title('PCE Cost', color='green', fontweight='bold')\n",
    "    ax.set_xlabel('Iteration'); ax.set_ylabel('Cost'); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = fig.add_subplot(gs[1, 1])\n",
    "    ax.plot(ngd['cost'], 'b-', lw=2)\n",
    "    ax.axhline(OPT_COST, color='red', ls='--', alpha=0.7, label=f'Opt={OPT_COST:.2f}')\n",
    "    ax.set_yscale('log'); ax.set_title('NGD Cost', color='blue', fontweight='bold')\n",
    "    ax.set_xlabel('Iteration'); ax.set_ylabel('Cost'); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = fig.add_subplot(gs[1, 2])\n",
    "    ax.plot(casadi_true, 'purple', lw=2)\n",
    "    ax.axhline(OPT_COST, color='red', ls='--', alpha=0.7, label=f'Opt={OPT_COST:.2f}')\n",
    "    ax.set_yscale('log'); ax.set_title('CasADi TRUE Cost', color='purple', fontweight='bold')\n",
    "    ax.set_xlabel('Iteration'); ax.set_ylabel('Cost'); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bottom row: Sigma evolution + Summary\n",
    "    ax = fig.add_subplot(gs[2, 0])\n",
    "    ax.plot(pce['sigma'], 'g-', lw=2)\n",
    "    ax.set_title(f'PCE σ ({cov_schedule_name})', color='green', fontweight='bold')\n",
    "    ax.set_xlabel('Iteration'); ax.set_ylabel('σ'); ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = fig.add_subplot(gs[2, 1])\n",
    "    ax.plot(ngd['sigma'], 'b-', lw=2)\n",
    "    ax.set_title(f'NGD σ ({cov_schedule_name})', color='blue', fontweight='bold')\n",
    "    ax.set_xlabel('Iteration'); ax.set_ylabel('σ'); ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary\n",
    "    ax = fig.add_subplot(gs[2, 2])\n",
    "    ax.axis('off')\n",
    "    summary = f'''\n",
    "════════════════════════════════\n",
    "RESULTS\n",
    "════════════════════════════════\n",
    "\n",
    "Optimal: x*={X_STAR:.4f}, J*={OPT_COST:.4f}\n",
    "\n",
    "PCE:    y={pce['y_best'][-1]:.4f}\n",
    "        cost={pce['cost_best'][-1]:.4f}\n",
    "\n",
    "NGD:    y={ngd['y'][-1]:.4f}\n",
    "        cost={ngd['cost'][-1]:.4f}\n",
    "\n",
    "CasADi: y={casadi['y'][-1]:.4f}\n",
    "        TRUE cost={casadi_true[-1]:.4f}\n",
    "\n",
    "════════════════════════════════\n",
    "Schedule: {cov_schedule_name}\n",
    "Decay Rate: {cov_decay_rate}\n",
    "'''\n",
    "    ax.text(0.05, 0.95, summary, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))\n",
    "    \n",
    "    plt.suptitle(f'Problem: {problem} | Start: y={y_init} | σ₀={sigma}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    interactive_comparison,\n",
    "    y_init=FloatSlider(min=-2, max=12, step=0.5, value=9.5, description='Start y:'),\n",
    "    sigma=FloatSlider(min=0.5, max=5, step=0.5, value=3, description='σ₀:'),\n",
    "    n_iterations=IntSlider(min=10, max=100, step=10, value=50, description='Iterations:'),\n",
    "    problem=Dropdown(options=['Quadratic', 'Two Barriers'], value='Two Barriers', description='Problem:'),\n",
    "    cov_schedule_name=Dropdown(options=['Constant', 'Linear', 'Exponential', 'Cosine'], value='Cosine', description='Schedule:'),\n",
    "    cov_decay_rate=FloatSlider(min=0.8, max=0.99, step=0.01, value=0.95, description='Decay Rate:')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "| Problem | PCE (Stable) | NGD | CasADi |\n",
    "|---------|--------------|-----|--------|\n",
    "| Quadratic | ✓ Optimal | ✓ Optimal | ✓ Optimal |\n",
    "| Two Barriers | ✓ Global (stochastic) | ✗ Local | ✗ Local (smooth approx) |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Simple problems**: All methods work\n",
    "2. **Barriers**: Gradient-based methods struggle with discontinuities\n",
    "3. **Smooth approximations**: Create artificial local minima\n",
    "4. **PCE advantages**: Evaluates TRUE cost, stochastic exploration\n",
    "5. **Best-so-far tracking**: Essential for PCE stability\n",
    "\n",
    "## Covariance Scheduling\n",
    "\n",
    "| Schedule | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Constant | Maximum exploration | May not converge tightly |\n",
    "| Linear | Simple, predictable | May decay too slowly/fast |\n",
    "| Exponential | Fast decay, good refinement | May lose exploration too early |\n",
    "| **Cosine** | Smooth transition, best of both | Slightly more complex |\n",
    "\n",
    "**Recommendation:** Use **Cosine** schedule for most problems - it provides good exploration early and smooth refinement later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
