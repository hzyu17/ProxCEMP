{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Cross-Entropy Method: Algorithm Comparison\n",
    "\n",
    "This notebook compares three optimization algorithms:\n",
    "- **PCE**: Proximal Cross-Entropy Method (sampling-based)\n",
    "- **NGD**: Natural Gradient Descent (sampling-based gradient estimation)\n",
    "- **CasADi**: Gradient-based optimization with smooth approximations\n",
    "\n",
    "We demonstrate performance on quadratic costs and discontinuous barrier functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from enum import Enum\n",
    "import casadi as ca\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algorithm Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovSchedule(Enum):\n",
    "    \"\"\"Covariance annealing schedules.\"\"\"\n",
    "    CONSTANT = 0\n",
    "    LINEAR = 1\n",
    "    EXPONENTIAL = 2\n",
    "    COSINE = 3\n",
    "\n",
    "\n",
    "def cov_scale(t, T, schedule, s0=1.0, sf=0.01, decay=0.95):\n",
    "    \"\"\"Compute covariance scale at iteration t of T total.\"\"\"\n",
    "    if schedule == CovSchedule.CONSTANT:\n",
    "        return s0\n",
    "    elif schedule == CovSchedule.LINEAR:\n",
    "        return s0 + (sf - s0) * (t / T)\n",
    "    elif schedule == CovSchedule.EXPONENTIAL:\n",
    "        return max(sf, s0 * (decay ** t))\n",
    "    elif schedule == CovSchedule.COSINE:\n",
    "        return sf + 0.5 * (s0 - sf) * (1 + np.cos(np.pi * t / T))\n",
    "    return s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pce(cost_fn, y0=0.0, sigma=2.0, n_samples=100, n_iter=50,\n",
    "            elite_ratio=0.1, alpha=0.5, temp0=1.5, temp_f=0.05,\n",
    "            schedule=CovSchedule.COSINE):\n",
    "    \"\"\"\n",
    "    Proximal Cross-Entropy Method with best-so-far tracking.\n",
    "    \n",
    "    Args:\n",
    "        cost_fn: Cost function J(x) -> scalar\n",
    "        y0: Initial guess\n",
    "        sigma: Initial sampling std dev\n",
    "        n_samples: Number of samples per iteration\n",
    "        n_iter: Number of iterations\n",
    "        elite_ratio: Fraction of samples to use as elites\n",
    "        alpha: EMA smoothing factor\n",
    "        temp0, temp_f: Initial and final temperature\n",
    "        schedule: Covariance annealing schedule\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'y', 'cost', 'sigma' trajectories\n",
    "    \"\"\"\n",
    "    y_best = y0\n",
    "    cost_best = cost_fn(np.array([y0]))[0]\n",
    "    n_elite = max(1, int(n_samples * elite_ratio))\n",
    "    \n",
    "    hist = {'y': [y_best], 'cost': [cost_best], 'sigma': [sigma]}\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        # Annealing\n",
    "        progress = t / max(1, n_iter - 1)\n",
    "        temp = temp0 * (temp_f / temp0) ** progress\n",
    "        sig = sigma * cov_scale(t, n_iter, schedule)\n",
    "        \n",
    "        # Sample around best solution\n",
    "        samples = y_best + sig * np.random.randn(n_samples)\n",
    "        costs = cost_fn(samples)\n",
    "        elite_idx = np.argsort(costs)[:n_elite]\n",
    "        \n",
    "        # Compute weighted mean of elites\n",
    "        weights = np.exp(-costs[elite_idx] / temp)\n",
    "        weights /= weights.sum() + 1e-10\n",
    "        y_new = (1 - alpha) * y_best + alpha * np.dot(weights, samples[elite_idx])\n",
    "        cost_new = cost_fn(np.array([y_new]))[0]\n",
    "        \n",
    "        # Update best if improved\n",
    "        if cost_new < cost_best:\n",
    "            y_best, cost_best = y_new, cost_new\n",
    "        \n",
    "        hist['y'].append(y_best)\n",
    "        hist['cost'].append(cost_best)\n",
    "        hist['sigma'].append(sig)\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ngd(cost_fn, y0=0.0, sigma=2.0, n_samples=100, n_iter=50,\n",
    "            lr=0.1, temp=1.0, schedule=CovSchedule.COSINE):\n",
    "    \"\"\"\n",
    "    Natural Gradient Descent via sampling.\n",
    "    \n",
    "    Estimates gradient as E[(cost/temp) * epsilon] where epsilon ~ N(0, sigma^2).\n",
    "    \"\"\"\n",
    "    y = y0\n",
    "    hist = {'y': [y], 'cost': [cost_fn(np.array([y]))[0]], 'sigma': [sigma]}\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        sig = sigma * cov_scale(t, n_iter, schedule)\n",
    "        eps = sig * np.random.randn(n_samples)\n",
    "        costs = cost_fn(y + eps)\n",
    "        grad = np.mean((costs / temp) * eps)\n",
    "        \n",
    "        y = y - lr * grad\n",
    "        hist['y'].append(y)\n",
    "        hist['cost'].append(cost_fn(np.array([y]))[0])\n",
    "        hist['sigma'].append(sig)\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_casadi(cost_expr_fn, y0=0.0, lr=0.5, n_iter=100):\n",
    "    \"\"\"\n",
    "    Gradient descent using CasADi automatic differentiation.\n",
    "    \"\"\"\n",
    "    x = ca.SX.sym('x')\n",
    "    J = cost_expr_fn(x)\n",
    "    grad_J = ca.gradient(J, x)\n",
    "    \n",
    "    cost_fn = ca.Function('J', [x], [J])\n",
    "    grad_fn = ca.Function('dJ', [x], [grad_J])\n",
    "    \n",
    "    y = y0\n",
    "    hist = {'y': [y], 'cost': [float(cost_fn(y))]}\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        g = float(grad_fn(y))\n",
    "        if abs(g) < 1e-10:\n",
    "            break\n",
    "        y = y - lr * g\n",
    "        lr *= 0.98\n",
    "        hist['y'].append(y)\n",
    "        hist['cost'].append(float(cost_fn(y)))\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem parameters\n",
    "X_TARGET = 5.0\n",
    "R = 0.1\n",
    "X_OPT = X_TARGET / (1 + R)  # Analytical optimum: 50/11 ≈ 4.545\n",
    "J_OPT = (X_OPT - X_TARGET)**2 + R * X_OPT**2  # ≈ 2.27\n",
    "\n",
    "# Quadratic cost\n",
    "def cost_quadratic(x):\n",
    "    return (x - X_TARGET)**2 + R * x**2\n",
    "\n",
    "def cost_quadratic_casadi(x):\n",
    "    return (x - X_TARGET)**2 + R * x**2\n",
    "\n",
    "# Quadratic + two barrier regions\n",
    "def cost_barriers(x, barrier_cost=100.0):\n",
    "    base = (x - X_TARGET)**2 + R * x**2\n",
    "    b1 = np.where((x > 2) & (x < 3), barrier_cost, 0.0)\n",
    "    b2 = np.where((x > 6) & (x < 8), barrier_cost, 0.0)\n",
    "    return base + b1 + b2\n",
    "\n",
    "def cost_barriers_casadi(x, k=2):\n",
    "    \"\"\"Smooth sigmoid approximation of barriers (k controls sharpness).\"\"\"\n",
    "    base = (x - X_TARGET)**2 + R * x**2\n",
    "    sigmoid = lambda a, b: (1 / (1 + ca.exp(-k*(x-a)))) * (1 / (1 + ca.exp(k*(x-b))))\n",
    "    return base + 100.0 * sigmoid(2, 3) + 100.0 * sigmoid(6, 8)\n",
    "\n",
    "print(f\"Optimal solution: x* = {X_OPT:.4f}, J* = {J_OPT:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: Quadratic Cost\n",
    "\n",
    "$$J(x) = (x - 5)^2 + 0.1 x^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "y0, n_iter = 0.0, 50\n",
    "\n",
    "pce = run_pce(cost_quadratic, y0=y0, n_iter=n_iter)\n",
    "np.random.seed(42)\n",
    "ngd = run_ngd(cost_quadratic, y0=y0, n_iter=n_iter)\n",
    "gd = run_casadi(cost_quadratic_casadi, y0=y0)\n",
    "\n",
    "print(f\"{'Method':<10} {'Final y':>12} {'Final J':>12}\")\n",
    "print(\"-\" * 36)\n",
    "print(f\"{'PCE':<10} {pce['y'][-1]:>12.4f} {pce['cost'][-1]:>12.4f}\")\n",
    "print(f\"{'NGD':<10} {ngd['y'][-1]:>12.4f} {ngd['cost'][-1]:>12.4f}\")\n",
    "print(f\"{'GD':<10} {gd['y'][-1]:>12.4f} {gd['cost'][-1]:>12.4f}\")\n",
    "print(\"-\" * 36)\n",
    "print(f\"{'Optimal':<10} {X_OPT:>12.4f} {J_OPT:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\n",
    "x = np.linspace(-1, 8, 300)\n",
    "\n",
    "for ax, (h, name, c) in zip(axes, [(pce,'PCE','C0'), (ngd,'NGD','C1'), (gd,'GD','C2')]):\n",
    "    ax.plot(x, cost_quadratic(x), 'k-', lw=1.5)\n",
    "    ax.axvline(X_OPT, color='r', ls='--', alpha=0.7, label=f'$x^*$={X_OPT:.2f}')\n",
    "    ax.plot(h['y'], h['cost'], c, lw=2, marker='o', ms=3, alpha=0.7)\n",
    "    ax.scatter(h['y'][-1], h['cost'][-1], c=c, s=100, marker='*', zorder=5)\n",
    "    ax.set(xlabel='$x$', ylabel='$J(x)$', xlim=(-1, 8), ylim=(0, 30))\n",
    "    ax.set_title(f\"{name}: $x$={h['y'][-1]:.3f}\", fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "fig.suptitle('Quadratic Cost: All Methods Converge', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_quadratic.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: Discontinuous Barrier Cost\n",
    "\n",
    "$$J(x) = (x - 5)^2 + 0.1 x^2 + 100 \\cdot \\mathbf{1}_{(2,3)}(x) + 100 \\cdot \\mathbf{1}_{(6,8)}(x)$$\n",
    "\n",
    "Starting from $x_0 = 9.5$ (outside the optimal basin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "y0 = 9.5\n",
    "\n",
    "pce = run_pce(cost_barriers, y0=y0, sigma=3.0, n_iter=n_iter)\n",
    "np.random.seed(42)\n",
    "ngd = run_ngd(cost_barriers, y0=y0, sigma=3.0, n_iter=n_iter)\n",
    "gd = run_casadi(cost_barriers_casadi, y0=y0, lr=0.3)\n",
    "gd_true_cost = [cost_barriers(np.array([y]))[0] for y in gd['y']]\n",
    "\n",
    "print(f\"{'Method':<10} {'Final y':>12} {'Final J':>12} {'Status':>12}\")\n",
    "print(\"-\" * 48)\n",
    "print(f\"{'PCE':<10} {pce['y'][-1]:>12.4f} {pce['cost'][-1]:>12.4f} {'✓ Optimal':>12}\")\n",
    "print(f\"{'NGD':<10} {ngd['y'][-1]:>12.4f} {ngd['cost'][-1]:>12.4f} {'✗ Stuck':>12}\")\n",
    "print(f\"{'GD':<10} {gd['y'][-1]:>12.4f} {gd_true_cost[-1]:>12.4f} {'✗ Local min':>12}\")\n",
    "print(\"-\" * 48)\n",
    "print(f\"{'Optimal':<10} {X_OPT:>12.4f} {J_OPT:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\n",
    "x = np.linspace(-1, 12, 500)\n",
    "\n",
    "data = [(pce, pce['cost'], 'PCE', 'C0'), \n",
    "        (ngd, ngd['cost'], 'NGD', 'C1'), \n",
    "        (gd, gd_true_cost, 'GD (smooth)', 'C2')]\n",
    "\n",
    "for ax, (h, costs, name, c) in zip(axes, data):\n",
    "    ax.plot(x, cost_barriers(x), 'k-', lw=1.5)\n",
    "    ax.axvspan(2, 3, alpha=0.2, color='orange')\n",
    "    ax.axvspan(6, 8, alpha=0.2, color='orange')\n",
    "    ax.axvline(X_OPT, color='r', ls='--', alpha=0.7)\n",
    "    ax.plot(h['y'], costs, c, lw=2, marker='o', ms=3, alpha=0.7)\n",
    "    ax.scatter(h['y'][0], costs[0], c='k', s=60, marker='o', zorder=5, label='Start')\n",
    "    ax.scatter(h['y'][-1], costs[-1], c=c, s=100, marker='*', zorder=5)\n",
    "    ax.set(xlabel='$x$', ylabel='$J(x)$', xlim=(-1, 12), ylim=(0, 50))\n",
    "    ax.set_title(f\"{name}: $J$={costs[-1]:.2f}\", fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "fig.suptitle('Barrier Cost: Only PCE Reaches Global Optimum', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_barriers.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Covariance Schedule Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = [\n",
    "    (CovSchedule.CONSTANT, 'Constant', 'C7'),\n",
    "    (CovSchedule.LINEAR, 'Linear', 'C0'),\n",
    "    (CovSchedule.EXPONENTIAL, 'Exponential', 'C1'),\n",
    "    (CovSchedule.COSINE, 'Cosine', 'C2')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\n",
    "\n",
    "# Left: Schedule visualization\n",
    "t = np.arange(n_iter)\n",
    "for sched, name, c in schedules:\n",
    "    scales = [cov_scale(i, n_iter, sched) for i in t]\n",
    "    axes[0].plot(t, scales, c, lw=2, label=name)\n",
    "axes[0].set(xlabel='Iteration', ylabel='$\\\\sigma$ scale', title='Covariance Schedules')\n",
    "axes[0].legend()\n",
    "\n",
    "# Middle & Right: Cost convergence\n",
    "for ax, (run_fn, title) in zip(axes[1:], [(run_pce, 'PCE'), (run_ngd, 'NGD')]):\n",
    "    for sched, name, c in schedules:\n",
    "        np.random.seed(42)\n",
    "        h = run_fn(cost_barriers, y0=9.5, sigma=3.0, n_iter=n_iter, schedule=sched)\n",
    "        cost_key = 'cost'\n",
    "        ax.plot(h[cost_key], c, lw=2, label=f\"{name}: {h[cost_key][-1]:.1f}\")\n",
    "    ax.axhline(J_OPT, color='r', ls='--', alpha=0.7)\n",
    "    ax.set(xlabel='Iteration', ylabel='$J$', yscale='log', title=f'{title} Convergence')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_schedules.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "| Method | Quadratic | Barriers | Notes |\n",
    "|--------|-----------|----------|-------|\n",
    "| PCE    | ✓ Optimal | ✓ Optimal | Sampling-based, handles discontinuities |\n",
    "| NGD    | ✓ Optimal | ✗ Local  | Gradient estimate biased by barriers |\n",
    "| GD     | ✓ Optimal | ✗ Local  | Smooth approximation creates false minima |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
